{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "FtzJCVBxGzIp"
   },
   "source": [
    "# Path setting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "f-tmQhMPGydo"
   },
   "outputs": [],
   "source": [
    "WORK_DIR = '/content/drive/MyDrive/RANZR/'\n",
    "SAVE_PATH = WORK_DIR"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "c0GfBy6ZKq9V"
   },
   "source": [
    "# Packages install"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "_nDCg2HNVeyZ",
    "outputId": "82f04dd3-264a-4147-803f-1fdb8ae99595"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting tf_clahe\n",
      "  Downloading https://files.pythonhosted.org/packages/bf/cd/0527c03ce45278628a1c478b428acb60d6772260e971afb99c65cab699a1/tf_clahe-0.1.0-py3-none-any.whl\n",
      "Collecting tensorflow-addons>=0.10\n",
      "  Downloading https://files.pythonhosted.org/packages/74/e3/56d2fe76f0bb7c88ed9b2a6a557e25e83e252aec08f13de34369cd850a0b/tensorflow_addons-0.12.1-cp37-cp37m-manylinux2010_x86_64.whl (703kB)\n",
      "Requirement already satisfied: tensorflow>=2.3 in /usr/local/lib/python3.7/dist-packages (from tf_clahe) (2.4.1)\n",
      "Requirement already satisfied: typeguard>=2.7 in /usr/local/lib/python3.7/dist-packages (from tensorflow-addons>=0.10->tf_clahe) (2.7.1)\n",
      "Requirement already satisfied: tensorboard~=2.4 in /usr/local/lib/python3.7/dist-packages (from tensorflow>=2.3->tf_clahe) (2.4.1)\n",
      "Requirement already satisfied: protobuf>=3.9.2 in /usr/local/lib/python3.7/dist-packages (from tensorflow>=2.3->tf_clahe) (3.12.4)\n",
      "Requirement already satisfied: google-pasta~=0.2 in /usr/local/lib/python3.7/dist-packages (from tensorflow>=2.3->tf_clahe) (0.2.0)\n",
      "Requirement already satisfied: typing-extensions~=3.7.4 in /usr/local/lib/python3.7/dist-packages (from tensorflow>=2.3->tf_clahe) (3.7.4.3)\n",
      "Requirement already satisfied: keras-preprocessing~=1.1.2 in /usr/local/lib/python3.7/dist-packages (from tensorflow>=2.3->tf_clahe) (1.1.2)\n",
      "Requirement already satisfied: wrapt~=1.12.1 in /usr/local/lib/python3.7/dist-packages (from tensorflow>=2.3->tf_clahe) (1.12.1)\n",
      "Requirement already satisfied: wheel~=0.35 in /usr/local/lib/python3.7/dist-packages (from tensorflow>=2.3->tf_clahe) (0.36.2)\n",
      "Requirement already satisfied: tensorflow-estimator<2.5.0,>=2.4.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow>=2.3->tf_clahe) (2.4.0)\n",
      "Requirement already satisfied: grpcio~=1.32.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow>=2.3->tf_clahe) (1.32.0)\n",
      "Requirement already satisfied: h5py~=2.10.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow>=2.3->tf_clahe) (2.10.0)\n",
      "Requirement already satisfied: gast==0.3.3 in /usr/local/lib/python3.7/dist-packages (from tensorflow>=2.3->tf_clahe) (0.3.3)\n",
      "Requirement already satisfied: astunparse~=1.6.3 in /usr/local/lib/python3.7/dist-packages (from tensorflow>=2.3->tf_clahe) (1.6.3)\n",
      "Requirement already satisfied: flatbuffers~=1.12.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow>=2.3->tf_clahe) (1.12)\n",
      "Requirement already satisfied: six~=1.15.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow>=2.3->tf_clahe) (1.15.0)\n",
      "Requirement already satisfied: absl-py~=0.10 in /usr/local/lib/python3.7/dist-packages (from tensorflow>=2.3->tf_clahe) (0.10.0)\n",
      "Requirement already satisfied: termcolor~=1.1.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow>=2.3->tf_clahe) (1.1.0)\n",
      "Requirement already satisfied: opt-einsum~=3.3.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow>=2.3->tf_clahe) (3.3.0)\n",
      "Requirement already satisfied: numpy~=1.19.2 in /usr/local/lib/python3.7/dist-packages (from tensorflow>=2.3->tf_clahe) (1.19.5)\n",
      "Requirement already satisfied: google-auth-oauthlib<0.5,>=0.4.1 in /usr/local/lib/python3.7/dist-packages (from tensorboard~=2.4->tensorflow>=2.3->tf_clahe) (0.4.2)\n",
      "Requirement already satisfied: google-auth<2,>=1.6.3 in /usr/local/lib/python3.7/dist-packages (from tensorboard~=2.4->tensorflow>=2.3->tf_clahe) (1.27.0)\n",
      "Requirement already satisfied: requests<3,>=2.21.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard~=2.4->tensorflow>=2.3->tf_clahe) (2.23.0)\n",
      "Requirement already satisfied: tensorboard-plugin-wit>=1.6.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard~=2.4->tensorflow>=2.3->tf_clahe) (1.8.0)\n",
      "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.7/dist-packages (from tensorboard~=2.4->tensorflow>=2.3->tf_clahe) (3.3.4)\n",
      "Requirement already satisfied: werkzeug>=0.11.15 in /usr/local/lib/python3.7/dist-packages (from tensorboard~=2.4->tensorflow>=2.3->tf_clahe) (1.0.1)\n",
      "Requirement already satisfied: setuptools>=41.0.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard~=2.4->tensorflow>=2.3->tf_clahe) (53.0.0)\n",
      "Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.7/dist-packages (from google-auth-oauthlib<0.5,>=0.4.1->tensorboard~=2.4->tensorflow>=2.3->tf_clahe) (1.3.0)\n",
      "Requirement already satisfied: rsa<5,>=3.1.4; python_version >= \"3.6\" in /usr/local/lib/python3.7/dist-packages (from google-auth<2,>=1.6.3->tensorboard~=2.4->tensorflow>=2.3->tf_clahe) (4.7.2)\n",
      "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.7/dist-packages (from google-auth<2,>=1.6.3->tensorboard~=2.4->tensorflow>=2.3->tf_clahe) (0.2.8)\n",
      "Requirement already satisfied: cachetools<5.0,>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from google-auth<2,>=1.6.3->tensorboard~=2.4->tensorflow>=2.3->tf_clahe) (4.2.1)\n",
      "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.21.0->tensorboard~=2.4->tensorflow>=2.3->tf_clahe) (1.24.3)\n",
      "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.21.0->tensorboard~=2.4->tensorflow>=2.3->tf_clahe) (2.10)\n",
      "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.21.0->tensorboard~=2.4->tensorflow>=2.3->tf_clahe) (3.0.4)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.21.0->tensorboard~=2.4->tensorflow>=2.3->tf_clahe) (2020.12.5)\n",
      "Requirement already satisfied: importlib-metadata; python_version < \"3.8\" in /usr/local/lib/python3.7/dist-packages (from markdown>=2.6.8->tensorboard~=2.4->tensorflow>=2.3->tf_clahe) (3.7.0)\n",
      "Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.7/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard~=2.4->tensorflow>=2.3->tf_clahe) (3.1.0)\n",
      "Requirement already satisfied: pyasn1>=0.1.3 in /usr/local/lib/python3.7/dist-packages (from rsa<5,>=3.1.4; python_version >= \"3.6\"->google-auth<2,>=1.6.3->tensorboard~=2.4->tensorflow>=2.3->tf_clahe) (0.4.8)\n",
      "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata; python_version < \"3.8\"->markdown>=2.6.8->tensorboard~=2.4->tensorflow>=2.3->tf_clahe) (3.4.0)\n",
      "Installing collected packages: tensorflow-addons, tf-clahe\n",
      "Successfully installed tensorflow-addons-0.12.1 tf-clahe-0.1.0\n",
      "Collecting git+https://github.com/qubvel/efficientnet\n",
      "  Cloning https://github.com/qubvel/efficientnet to /tmp/pip-req-build-78276tg2\n",
      "Collecting keras_applications<=1.0.8,>=1.0.7\n",
      "  Downloading https://files.pythonhosted.org/packages/71/e3/19762fdfc62877ae9102edf6342d71b28fbfd9dea3d2f96a882ce099b03f/Keras_Applications-1.0.8-py3-none-any.whl (50kB)\n",
      "Requirement already satisfied, skipping upgrade: scikit-image in /usr/local/lib/python3.7/dist-packages (from efficientnet==1.1.1) (0.16.2)\n",
      "Requirement already satisfied, skipping upgrade: h5py in /usr/local/lib/python3.7/dist-packages (from keras_applications<=1.0.8,>=1.0.7->efficientnet==1.1.1) (2.10.0)\n",
      "Requirement already satisfied, skipping upgrade: numpy>=1.9.1 in /usr/local/lib/python3.7/dist-packages (from keras_applications<=1.0.8,>=1.0.7->efficientnet==1.1.1) (1.19.5)\n",
      "Requirement already satisfied, skipping upgrade: networkx>=2.0 in /usr/local/lib/python3.7/dist-packages (from scikit-image->efficientnet==1.1.1) (2.5)\n",
      "Requirement already satisfied, skipping upgrade: scipy>=0.19.0 in /usr/local/lib/python3.7/dist-packages (from scikit-image->efficientnet==1.1.1) (1.4.1)\n",
      "Requirement already satisfied, skipping upgrade: matplotlib!=3.0.0,>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from scikit-image->efficientnet==1.1.1) (3.2.2)\n",
      "Requirement already satisfied, skipping upgrade: imageio>=2.3.0 in /usr/local/lib/python3.7/dist-packages (from scikit-image->efficientnet==1.1.1) (2.4.1)\n",
      "Requirement already satisfied, skipping upgrade: pillow>=4.3.0 in /usr/local/lib/python3.7/dist-packages (from scikit-image->efficientnet==1.1.1) (7.0.0)\n",
      "Requirement already satisfied, skipping upgrade: PyWavelets>=0.4.0 in /usr/local/lib/python3.7/dist-packages (from scikit-image->efficientnet==1.1.1) (1.1.1)\n",
      "Requirement already satisfied, skipping upgrade: six in /usr/local/lib/python3.7/dist-packages (from h5py->keras_applications<=1.0.8,>=1.0.7->efficientnet==1.1.1) (1.15.0)\n",
      "Requirement already satisfied, skipping upgrade: decorator>=4.3.0 in /usr/local/lib/python3.7/dist-packages (from networkx>=2.0->scikit-image->efficientnet==1.1.1) (4.4.2)\n",
      "Requirement already satisfied, skipping upgrade: cycler>=0.10 in /usr/local/lib/python3.7/dist-packages (from matplotlib!=3.0.0,>=2.0.0->scikit-image->efficientnet==1.1.1) (0.10.0)\n",
      "Requirement already satisfied, skipping upgrade: python-dateutil>=2.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib!=3.0.0,>=2.0.0->scikit-image->efficientnet==1.1.1) (2.8.1)\n",
      "Requirement already satisfied, skipping upgrade: kiwisolver>=1.0.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib!=3.0.0,>=2.0.0->scikit-image->efficientnet==1.1.1) (1.3.1)\n",
      "Requirement already satisfied, skipping upgrade: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib!=3.0.0,>=2.0.0->scikit-image->efficientnet==1.1.1) (2.4.7)\n",
      "Building wheels for collected packages: efficientnet\n",
      "  Building wheel for efficientnet (setup.py): started\n",
      "  Building wheel for efficientnet (setup.py): finished with status 'done'\n",
      "  Created wheel for efficientnet: filename=efficientnet-1.1.1-cp37-none-any.whl size=18421 sha256=343ca9458f581b35e0a940ba7ec452d4d97a21d35ce6ddcce2fbe9827d2e70a2\n",
      "  Stored in directory: /tmp/pip-ephem-wheel-cache-brs2xj9m/wheels/64/60/2e/30ebaa76ed1626e86bfb0cc0579b737fdb7d9ff8cb9522663a\n",
      "Successfully built efficientnet\n",
      "Installing collected packages: keras-applications, efficientnet\n",
      "Successfully installed efficientnet-1.1.1 keras-applications-1.0.8\n",
      "Collecting adabelief-tf\n",
      "  Downloading https://files.pythonhosted.org/packages/39/54/baa03c4836ff9257150e362a2c722d6b10440e200fb5880dc1288d79d459/adabelief_tf-0.2.1-py3-none-any.whl\n",
      "Collecting colorama>=0.4.0\n",
      "  Downloading https://files.pythonhosted.org/packages/44/98/5b86278fbbf250d239ae0ecb724f8572af1c91f4a11edf4d36a206189440/colorama-0.4.4-py2.py3-none-any.whl\n",
      "Requirement already satisfied: tabulate>=0.7 in /usr/local/lib/python3.7/dist-packages (from adabelief-tf) (0.8.9)\n",
      "Requirement already satisfied: tensorflow>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from adabelief-tf) (2.4.1)\n",
      "Requirement already satisfied: gast==0.3.3 in /usr/local/lib/python3.7/dist-packages (from tensorflow>=2.0.0->adabelief-tf) (0.3.3)\n",
      "Requirement already satisfied: tensorboard~=2.4 in /usr/local/lib/python3.7/dist-packages (from tensorflow>=2.0.0->adabelief-tf) (2.4.1)\n",
      "Requirement already satisfied: termcolor~=1.1.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow>=2.0.0->adabelief-tf) (1.1.0)\n",
      "Requirement already satisfied: opt-einsum~=3.3.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow>=2.0.0->adabelief-tf) (3.3.0)\n",
      "Requirement already satisfied: tensorflow-estimator<2.5.0,>=2.4.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow>=2.0.0->adabelief-tf) (2.4.0)\n",
      "Requirement already satisfied: grpcio~=1.32.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow>=2.0.0->adabelief-tf) (1.32.0)\n",
      "Requirement already satisfied: astunparse~=1.6.3 in /usr/local/lib/python3.7/dist-packages (from tensorflow>=2.0.0->adabelief-tf) (1.6.3)\n",
      "Requirement already satisfied: wrapt~=1.12.1 in /usr/local/lib/python3.7/dist-packages (from tensorflow>=2.0.0->adabelief-tf) (1.12.1)\n",
      "Requirement already satisfied: keras-preprocessing~=1.1.2 in /usr/local/lib/python3.7/dist-packages (from tensorflow>=2.0.0->adabelief-tf) (1.1.2)\n",
      "Requirement already satisfied: protobuf>=3.9.2 in /usr/local/lib/python3.7/dist-packages (from tensorflow>=2.0.0->adabelief-tf) (3.12.4)\n",
      "Requirement already satisfied: typing-extensions~=3.7.4 in /usr/local/lib/python3.7/dist-packages (from tensorflow>=2.0.0->adabelief-tf) (3.7.4.3)\n",
      "Requirement already satisfied: numpy~=1.19.2 in /usr/local/lib/python3.7/dist-packages (from tensorflow>=2.0.0->adabelief-tf) (1.19.5)\n",
      "Requirement already satisfied: six~=1.15.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow>=2.0.0->adabelief-tf) (1.15.0)\n",
      "Requirement already satisfied: wheel~=0.35 in /usr/local/lib/python3.7/dist-packages (from tensorflow>=2.0.0->adabelief-tf) (0.36.2)\n",
      "Requirement already satisfied: h5py~=2.10.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow>=2.0.0->adabelief-tf) (2.10.0)\n",
      "Requirement already satisfied: google-pasta~=0.2 in /usr/local/lib/python3.7/dist-packages (from tensorflow>=2.0.0->adabelief-tf) (0.2.0)\n",
      "Requirement already satisfied: absl-py~=0.10 in /usr/local/lib/python3.7/dist-packages (from tensorflow>=2.0.0->adabelief-tf) (0.10.0)\n",
      "Requirement already satisfied: flatbuffers~=1.12.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow>=2.0.0->adabelief-tf) (1.12)\n",
      "Requirement already satisfied: tensorboard-plugin-wit>=1.6.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard~=2.4->tensorflow>=2.0.0->adabelief-tf) (1.8.0)\n",
      "Requirement already satisfied: requests<3,>=2.21.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard~=2.4->tensorflow>=2.0.0->adabelief-tf) (2.23.0)\n",
      "Requirement already satisfied: werkzeug>=0.11.15 in /usr/local/lib/python3.7/dist-packages (from tensorboard~=2.4->tensorflow>=2.0.0->adabelief-tf) (1.0.1)\n",
      "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.7/dist-packages (from tensorboard~=2.4->tensorflow>=2.0.0->adabelief-tf) (3.3.4)\n",
      "Requirement already satisfied: google-auth-oauthlib<0.5,>=0.4.1 in /usr/local/lib/python3.7/dist-packages (from tensorboard~=2.4->tensorflow>=2.0.0->adabelief-tf) (0.4.2)\n",
      "Requirement already satisfied: setuptools>=41.0.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard~=2.4->tensorflow>=2.0.0->adabelief-tf) (53.0.0)\n",
      "Requirement already satisfied: google-auth<2,>=1.6.3 in /usr/local/lib/python3.7/dist-packages (from tensorboard~=2.4->tensorflow>=2.0.0->adabelief-tf) (1.27.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.21.0->tensorboard~=2.4->tensorflow>=2.0.0->adabelief-tf) (2020.12.5)\n",
      "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.21.0->tensorboard~=2.4->tensorflow>=2.0.0->adabelief-tf) (3.0.4)\n",
      "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.21.0->tensorboard~=2.4->tensorflow>=2.0.0->adabelief-tf) (1.24.3)\n",
      "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.21.0->tensorboard~=2.4->tensorflow>=2.0.0->adabelief-tf) (2.10)\n",
      "Requirement already satisfied: importlib-metadata; python_version < \"3.8\" in /usr/local/lib/python3.7/dist-packages (from markdown>=2.6.8->tensorboard~=2.4->tensorflow>=2.0.0->adabelief-tf) (3.7.0)\n",
      "Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.7/dist-packages (from google-auth-oauthlib<0.5,>=0.4.1->tensorboard~=2.4->tensorflow>=2.0.0->adabelief-tf) (1.3.0)\n",
      "Requirement already satisfied: cachetools<5.0,>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from google-auth<2,>=1.6.3->tensorboard~=2.4->tensorflow>=2.0.0->adabelief-tf) (4.2.1)\n",
      "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.7/dist-packages (from google-auth<2,>=1.6.3->tensorboard~=2.4->tensorflow>=2.0.0->adabelief-tf) (0.2.8)\n",
      "Requirement already satisfied: rsa<5,>=3.1.4; python_version >= \"3.6\" in /usr/local/lib/python3.7/dist-packages (from google-auth<2,>=1.6.3->tensorboard~=2.4->tensorflow>=2.0.0->adabelief-tf) (4.7.2)\n",
      "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata; python_version < \"3.8\"->markdown>=2.6.8->tensorboard~=2.4->tensorflow>=2.0.0->adabelief-tf) (3.4.0)\n",
      "Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.7/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard~=2.4->tensorflow>=2.0.0->adabelief-tf) (3.1.0)\n",
      "Requirement already satisfied: pyasn1<0.5.0,>=0.4.6 in /usr/local/lib/python3.7/dist-packages (from pyasn1-modules>=0.2.1->google-auth<2,>=1.6.3->tensorboard~=2.4->tensorflow>=2.0.0->adabelief-tf) (0.4.8)\n",
      "Installing collected packages: colorama, adabelief-tf\n",
      "Successfully installed adabelief-tf-0.2.1 colorama-0.4.4\n",
      "Collecting keras-adabound\n",
      "  Downloading https://files.pythonhosted.org/packages/bf/74/85de8379eba8e0f819ef9b62ff32d24a3f624758800e12bd9572e3afb546/keras-adabound-0.6.0.tar.gz\n",
      "Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from keras-adabound) (1.19.5)\n",
      "Requirement already satisfied: Keras in /usr/local/lib/python3.7/dist-packages (from keras-adabound) (2.4.3)\n",
      "Requirement already satisfied: typeguard in /usr/local/lib/python3.7/dist-packages (from keras-adabound) (2.7.1)\n",
      "Requirement already satisfied: h5py in /usr/local/lib/python3.7/dist-packages (from Keras->keras-adabound) (2.10.0)\n",
      "Requirement already satisfied: scipy>=0.14 in /usr/local/lib/python3.7/dist-packages (from Keras->keras-adabound) (1.4.1)\n",
      "Requirement already satisfied: pyyaml in /usr/local/lib/python3.7/dist-packages (from Keras->keras-adabound) (3.13)\n",
      "Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from h5py->Keras->keras-adabound) (1.15.0)\n",
      "Building wheels for collected packages: keras-adabound\n",
      "  Building wheel for keras-adabound (setup.py): started\n",
      "  Building wheel for keras-adabound (setup.py): finished with status 'done'\n",
      "  Created wheel for keras-adabound: filename=keras_adabound-0.6.0-cp37-none-any.whl size=6610 sha256=6c4c116304e4cd1baea0c7b773208f6a3c4dfe1f190013ee3476d882191111d0\n",
      "  Stored in directory: /root/.cache/pip/wheels/f1/81/9c/04af926d62bddd280c97af1704a9baaef511664b56865958e8\n",
      "Successfully built keras-adabound\n",
      "Installing collected packages: keras-adabound\n",
      "Successfully installed keras-adabound-0.6.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  Running command git clone -q https://github.com/qubvel/efficientnet /tmp/pip-req-build-78276tg2\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "pip install tf_clahe\n",
    "pip install -U git+https://github.com/qubvel/efficientnet\n",
    "pip install adabelief-tf\n",
    "pip install keras-adabound"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "bOCwoQfIKtgu"
   },
   "source": [
    "# Google drive"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ZyWwPjsFdl2h",
    "outputId": "846a8aa7-a794-41ae-fd1a-8eee728a1b9e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mounted at /content/drive\n"
     ]
    }
   ],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Otv5R5PfKu8H"
   },
   "source": [
    "# Import libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "33Z-MXEvdHVU"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import models, layers\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint, EarlyStopping, ReduceLROnPlateau\n",
    "from tensorflow.keras.applications import Xception\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from functools import partial\n",
    "import tensorflow_addons as tfa\n",
    "\n",
    "import tf_clahe\n",
    "import efficientnet.keras as efn \n",
    "from keras_adabound import AdaBound\n",
    "from adabelief_tf import AdaBeliefOptimizer\n",
    "\n",
    "# ignoring warnings\n",
    "import warnings\n",
    "warnings.simplefilter(\"ignore\")\n",
    "\n",
    "import os, cv2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "mXQmn6uTK4tn"
   },
   "source": [
    "# Dataset preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "iDK-LmB1dHVW"
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GroupKFold\n",
    "\n",
    "# Train test split\n",
    "\n",
    "gkf = GroupKFold(5)\n",
    "\n",
    "df = pd.read_csv(WORK_DIR + 'train.csv')\n",
    "df = df.sample(frac=1, random_state = 69).reset_index(drop=True)\n",
    "\n",
    "label_cols = df.columns[1:-1]\n",
    "\n",
    "folds = gkf.split(df['StudyInstanceUID'], df[df.columns[1:-1]], groups=df['PatientID'])\n",
    "df_folds = []\n",
    "\n",
    "for fold in folds:\n",
    "    df_folds.append(pd.DataFrame(df.loc[fold[1]]))\n",
    "    \n",
    "\n",
    "\n",
    "df_train = df_folds.pop(0)\n",
    "for i in range(0, len(df_folds)-1):\n",
    "    df_train = df_train.append(df_folds.pop(0))\n",
    "\n",
    "df_valid = df_folds.pop(0)\n",
    "\n",
    "\n",
    "train_paths = WORK_DIR + \"/train/\" + df_train['StudyInstanceUID'] + '.jpg'\n",
    "valid_paths = WORK_DIR + \"/train/\" + df_valid['StudyInstanceUID'] + '.jpg'\n",
    "train_labels = df_train.drop(columns = [\"StudyInstanceUID\", \"PatientID\"]).values\n",
    "valid_labels = df_valid.drop(columns = [\"StudyInstanceUID\", \"PatientID\"]).values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "D8L-BigXdHVX"
   },
   "outputs": [],
   "source": [
    "# Main parameters\n",
    "BATCH_SIZE = 8\n",
    "STEPS_PER_EPOCH = len(train_paths) // BATCH_SIZE\n",
    "VALIDATION_STEPS = len(valid_paths) // BATCH_SIZE\n",
    "EPOCHS = 30\n",
    "SIZE = (600, 600, 3)\n",
    "NUM_CLASSES = len(label_cols)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "VfPiWPQbcQYA"
   },
   "source": [
    "# Label Weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "x4zBEd6c1Zrr"
   },
   "outputs": [],
   "source": [
    "label_weights = pd.read_csv(WORK_DIR + '/weights.csv', names=[\"0\", \"1\"]).to_numpy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-5hHQ7oDKlRy"
   },
   "source": [
    "# Augmentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "_FO-zmcOEQOa"
   },
   "outputs": [],
   "source": [
    "def multiplicative_noise(img, multiplier=[0.95, 1.05]):\n",
    "    img = img * tf.random.uniform(shape = SIZE,  minval = multiplier[0], maxval = multiplier[1], dtype=tf.float32)\n",
    "    return  img\n",
    "\n",
    "\n",
    "augmentations = [\n",
    "        [partial(tf.image.flip_left_right)],\n",
    "        [partial(tf.image.random_contrast, lower = 0.8, upper = 1.2)],\n",
    "        [partial(tf.image.random_brightness, max_delta = 0.2)],\n",
    "        [partial(tf.image.random_saturation, lower = 0.8, upper = 1.2)],\n",
    "        [partial(multiplicative_noise)]\n",
    "    ]\n",
    "augmentations = np.array(augmentations, dtype=object).reshape(-1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "nwtvzFukLFyS"
   },
   "source": [
    "# Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "AxqE1wisdHVX"
   },
   "outputs": [],
   "source": [
    "def build_decoder(with_labels = True,\n",
    "                  target_size = SIZE[:2], \n",
    "                  ext = 'jpg'):\n",
    "    def decode(path):\n",
    "        file_bytes = tf.io.read_file(path)\n",
    "        if ext == 'png':\n",
    "            img = tf.image.decode_png(file_bytes, channels = 3)\n",
    "        elif ext in ['jpg', 'jpeg']:\n",
    "            img = tf.image.decode_jpeg(file_bytes, channels = 3)\n",
    "        else:\n",
    "            raise ValueError(\"Image extension not supported\")\n",
    "\n",
    "        \n",
    "        img = tf.image.resize(img, target_size)\n",
    "        img = tf_clahe._clahe(img, clip_limit = 4, tile_grid_size = [8,8], gpu_optimized = True)\n",
    "        img = tf.cast(img, tf.float32) / 255.0\n",
    "\n",
    "        return img\n",
    "    \n",
    "    def decode_with_labels(path, label):\n",
    "        return (decode(path), tf.cast(label, dtype=tf.float32))\n",
    "    \n",
    "    return decode_with_labels if with_labels else decode\n",
    "\n",
    "\n",
    "\n",
    "def set_shapes(img, label, img_shape=SIZE):\n",
    "    print(img)\n",
    "    img.set_shape([img_shape[0], img_shape[1], img_shape[2]])\n",
    "    label.set_shape([NUM_CLASSES])\n",
    "    return img, label\n",
    "\n",
    "\n",
    "\n",
    "@tf.function\n",
    "def augmentation(imgs, label):\n",
    "    array = np.random.choice(augmentations, replace = False, size = np.random.randint(1,len(augmentations)))\n",
    "    for element in array.tolist():\n",
    "        imgs = element(imgs)\n",
    "    imgs = tf.clip_by_value(imgs, 0., 1.)\n",
    "    \n",
    "    return imgs, label\n",
    "\n",
    "\n",
    "\n",
    "def build_dataset(paths, labels = None, bsize = 32, cache = True,\n",
    "                  decode_fn = None, augment_fn = None,\n",
    "                  augment = True,\n",
    "                  cache_dir = \"\"):\n",
    "    \n",
    "    if cache_dir != \"\" and cache is True:\n",
    "        os.makedirs(cache_dir, exist_ok=True)\n",
    "    \n",
    "    if decode_fn is None:\n",
    "        decode_fn = build_decoder(labels is not None)\n",
    "    \n",
    "    AUTO = tf.data.experimental.AUTOTUNE\n",
    "    slices = paths if labels is None else (paths, labels)\n",
    "    \n",
    "    dset = tf.data.Dataset.from_tensor_slices(slices)\n",
    "    dset = dset.map(decode_fn, num_parallel_calls = AUTO)\n",
    "    dset = dset.cache(cache_dir) if cache else dset\n",
    "    dset = dset.map(augmentation, num_parallel_calls = AUTO) if augment else dset  \n",
    "    dset = dset.map(set_shapes, num_parallel_calls = AUTO).prefetch(AUTO)\n",
    "    dset = dset.batch(bsize, drop_remainder= True)\n",
    "    return dset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "K3FR_a2EdHVY"
   },
   "outputs": [],
   "source": [
    "# Tensorflow datasets\n",
    "train_df = build_dataset(\n",
    "    train_paths, train_labels, bsize = BATCH_SIZE, \n",
    "    cache = True, augment = True)\n",
    "\n",
    "\n",
    "valid_df = build_dataset(\n",
    "    valid_paths, valid_labels, bsize = BATCH_SIZE, augment = False, \n",
    "    cache = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2u45U93hfLHi"
   },
   "source": [
    "# Custom Loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "YYzHrTacfTd5"
   },
   "outputs": [],
   "source": [
    "import tensorflow.keras.backend as K\n",
    "import math\n",
    "\n",
    "\n",
    "def bce_l1(l1 = 1):\n",
    "    def bce1(y_true,y_pred):\n",
    "        abs_diff = K.abs(y_true-y_pred)\n",
    "        return K.sum(\n",
    "            ((K.binary_crossentropy(y_true,y_pred)) * (((1-y_true)*label_weights[:,0])+(y_true*label_weights[:,1]))) + l1*abs_diff + K.square(abs_diff)\n",
    "        ,axis = 0)\n",
    "    return bce1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "aTB8uNdaLNIC"
   },
   "source": [
    "# EfficientNet B2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "akD4WK8bIKWj"
   },
   "outputs": [],
   "source": [
    "def create_model(conv_base):\n",
    "    model = conv_base.output\n",
    "    model = layers.GlobalAveragePooling2D()(model)\n",
    "    model = layers.Dropout(0.3)(model)\n",
    "    model = layers.Dense(11, activation = \"sigmoid\")(model)\n",
    "    model = tf.keras.models.Model(conv_base.input, model, name = conv_base._name)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "b5j5m8zIdHVY"
   },
   "outputs": [],
   "source": [
    "import efficientnet.keras as efn \n",
    "\n",
    "\n",
    "backbone = efn.EfficientNetB2(\n",
    "    include_top = False,\n",
    "    weights = 'noisy-student',\n",
    "    input_shape = SIZE\n",
    ")\n",
    "backbone._name = 'EfficientNetB2'\n",
    "model = create_model(backbone)\n",
    "    #model.load_weights('/content/drive/MyDrive/RANZR/Final/EfficientNetB2/H5/Finale.h5')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "V9RmP9fUYVwl"
   },
   "source": [
    "# Area MCC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "1xgvK1osY7O5",
    "outputId": "81c337c4-204b-4d3c-e027-0974a9fc5d54"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defining AreaMCC\n"
     ]
    }
   ],
   "source": [
    "\"\"\"Matthews Correlation Coefficient Implementation.\"\"\"\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import backend as K\n",
    "\n",
    "from tensorflow_addons.utils.types import AcceptableDTypes, FloatTensorLike, TensorLike\n",
    "from typeguard import typechecked\n",
    "#del globals()['AreaMCC'] \n",
    "if globals().get(\"AreaMCC\") is None:\n",
    "  \n",
    "  print('Defining AreaMCC')\n",
    "  class AreaMCC(tf.keras.metrics.Metric):\n",
    "      @typechecked\n",
    "      def __init__(\n",
    "          self,\n",
    "          num_labels: FloatTensorLike = 1,\n",
    "          num_threshold: int = 200,\n",
    "          start: float = 0.0,\n",
    "          stop: float = 1.0,\n",
    "          name: str = \"AreaMCC\",\n",
    "          dtype: AcceptableDTypes = None,\n",
    "          **kwargs,\n",
    "      ):\n",
    "          \"\"\"Creates a Matthews Correlation Coefficient instance.\"\"\"\n",
    "          super().__init__(name=name, dtype=dtype)\n",
    "          self.num_labels = num_labels\n",
    "          self.num_threshold = num_threshold\n",
    "          self.start = start \n",
    "          self.stop = stop\n",
    "          self.mcc = self.add_weight(\n",
    "              \"MCC\",\n",
    "              shape=[self.num_labels],\n",
    "              initializer=\"zeros\",\n",
    "              dtype=self.dtype,\n",
    "          )\n",
    "\n",
    "\n",
    "      def update_confusion_matrix_value(self, y_true, y_pred):\n",
    "          true_positive = tf.math.count_nonzero(y_true * y_pred, 0)\n",
    "          # true_negative\n",
    "          y_true_negative = tf.math.not_equal(y_true, 1.0)\n",
    "          y_pred_negative = tf.math.not_equal(y_pred, 1.0)\n",
    "          true_negative = tf.math.count_nonzero(\n",
    "              tf.math.logical_and(y_true_negative, y_pred_negative), axis=0\n",
    "          )\n",
    "          # predicted sum\n",
    "          pred_sum = tf.math.count_nonzero(y_pred, 0)\n",
    "          # Ground truth label sum\n",
    "          true_sum = tf.math.count_nonzero(y_true, 0)\n",
    "          false_positive = pred_sum - true_positive\n",
    "          false_negative = true_sum - true_positive\n",
    "\n",
    "          numerator1 = true_positive * true_negative\n",
    "          numerator2 = false_positive * false_negative\n",
    "          numerator = tf.cast(numerator1 - numerator2, dtype = self.dtype)\n",
    "          # denominator\n",
    "          denominator1 = tf.cast(true_positive + false_positive, dtype = self.dtype)\n",
    "          denominator2 = tf.cast(true_positive + false_negative, dtype = self.dtype)\n",
    "          denominator3 = tf.cast(true_negative + false_positive, dtype = self.dtype)\n",
    "          denominator4 = tf.cast(true_negative + false_negative, dtype = self.dtype)\n",
    "          \n",
    "          denominator = tf.math.sqrt(\n",
    "              denominator1 * denominator2 * denominator3 * denominator4\n",
    "          )\n",
    "\n",
    "          \n",
    "          mcc = tf.math.divide_no_nan(numerator, denominator)\n",
    "          #print(mcc)\n",
    "          return mcc\n",
    "\n",
    "      # TODO: sample_weights\n",
    "      def update_state(self, y_true, y_pred, sample_weight=None):\n",
    "          y_true = tf.cast(y_true, dtype=self.dtype)\n",
    "          y_pred = tf.cast(y_pred, dtype=self.dtype)\n",
    "          \n",
    "          # self.start = tf.cast(self.start, dtype = self.dtype)\n",
    "          # self.stop = tf.cast(self.stop, dtype = self.dtype)\n",
    "\n",
    "          step = tf.constant( (self.stop-self.start) / self.num_threshold)\n",
    "\n",
    "          i = tf.constant(0, dtype=tf.int32)\n",
    "          mcc = tf.zeros([self.num_labels],dtype=tf.float32)\n",
    "\n",
    "          cond = lambda i, mcc : i < self.num_threshold\n",
    "          body = lambda i, mcc : [i+1, \n",
    "                                  tf.add(mcc, \n",
    "                                      self.update_confusion_matrix_value(y_true, tf.cast(y_pred > (self.start + (tf.cast(i, tf.float32)*step)), dtype=self.dtype)) * step ) ]\n",
    "          a = tf.while_loop(cond, body, [i, mcc])\n",
    "          self.mcc.assign_add(tf.cast(a[1],self.dtype))\n",
    "\n",
    "      def result(self):\n",
    "          return tf.reduce_mean(self.mcc)\n",
    "\n",
    "      def get_config(self):\n",
    "          \"\"\"Returns the serializable config of the metric.\"\"\"\n",
    "\n",
    "          config = {\n",
    "              \"num_labels\": self.num_labels,\n",
    "              \"num_threshold\": self.num_threshold\n",
    "          }\n",
    "          base_config = super().get_config()\n",
    "          return {**base_config, **config}\n",
    "\n",
    "      def reset_states(self):\n",
    "          \"\"\"Resets all of the metric state variables.\"\"\"\n",
    "          reset_value = np.zeros(self.num_labels, dtype=self.dtype)\n",
    "          K.batch_set_value([(v, reset_value) for v in self.variables])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "cmi2iOZtyxL7"
   },
   "source": [
    "# Compute metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "XIzbkFXyyzsx"
   },
   "outputs": [],
   "source": [
    "from time import time\n",
    "\n",
    "def compute_metrics(model, dataset, mode = 'train', start_ = 0.0, normalize = True):\n",
    "    print(start_, 'start')\n",
    "    s = 'Computing metrics %s dataset' % mode\n",
    "    print('-' * len(s))\n",
    "    print(s)\n",
    "    print('-' * len(s))\n",
    "    metrics = np.zeros((3,12))\n",
    "    start = time()\n",
    "    y_pred = model.predict(dataset) \n",
    "    y_pred = y_pred.astype(np.float32)\n",
    "    if mode == 'train':\n",
    "        y_true = np.array(train_labels[:STEPS_PER_EPOCH * BATCH_SIZE, :]) \n",
    "    else:\n",
    "        y_true = np.array(valid_labels[:VALIDATION_STEPS * BATCH_SIZE, :]) \n",
    "    \n",
    "    mcc = AreaMCC(num_labels=1, num_threshold = 200, start = 0.0)\n",
    "    pmcc = AreaMCC(num_labels=1, num_threshold = 200, start = start_)\n",
    "    auc = tf.keras.metrics.AUC(num_thresholds = 200, multi_label = False)\n",
    "    \n",
    "    for j in range(len(y_true[0])):\n",
    "        # --- AUC PER SINGOLA LABEL ---\n",
    "        auc.update_state(y_true[:,j], y_pred[:,j])\n",
    "        metrics[0,j] = auc.result().numpy()\n",
    "        auc.reset_states()\n",
    "\n",
    "        # --- AREAMCC PER SINGOLA LABEL ---\n",
    "        \n",
    "        mcc.update_state(y_true[:,j], y_pred[:,j])\n",
    "        metrics[1,j] = mcc.result().numpy()\n",
    "        mcc.reset_states()\n",
    "\n",
    "         # --- PARTIAL AREAMCC PER SINGOLA LABEL ---\n",
    "        pmcc.update_state(y_true[:,j], y_pred[:,j])\n",
    "        metrics[2,j] = pmcc.result().numpy()\n",
    "        print(metrics[2,j])\n",
    "        pmcc.reset_states()\n",
    "\n",
    "        if normalize:\n",
    "          metrics[1,j] = (metrics[1,j]+1)/2\n",
    "          metrics[2,j] = metrics[2,j] + 1/2\n",
    "        \n",
    "    # --- AUC TOTALE ---\n",
    "    auc = tf.keras.metrics.AUC(multi_label=True)\n",
    "    auc.update_state(y_true,y_pred)\n",
    "    metrics[0,11] = auc.result().numpy()\n",
    "\n",
    "    # --- AREAMCC TOTALE ---\n",
    "    mcc = AreaMCC(num_labels=11, num_threshold=200)\n",
    "    mcc.update_state(y_true, y_pred)\n",
    "    metrics[1,11] = mcc.result().numpy()\n",
    "\n",
    "    # ---PARTIAL AREAMCC TOTALE ---\n",
    "    pmcc = AreaMCC(num_labels=11, num_threshold=200, start = start_)\n",
    "    pmcc.update_state(y_true, y_pred)\n",
    "    metrics[2,11] = pmcc.result().numpy()\n",
    "    \n",
    "    if normalize:\n",
    "      metrics[1,11] = (metrics[1,11]+1) /2\n",
    "      metrics[2,11] = metrics[2,11] + 1/2\n",
    "\n",
    "\n",
    "    print(\"Metrics computed in %.3f seconds\" % (time()-start))\n",
    "    return metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "gsTF8dRwXfAd"
   },
   "source": [
    "# Training "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "sydSyFGXXfAd"
   },
   "outputs": [],
   "source": [
    "SAVE_PATH = WORK_DIR + 'Final/'\n",
    "makedir = True\n",
    "dirs = os.listdir(SAVE_PATH)\n",
    "for dir in dirs:\n",
    "    if model._name not in dir:\n",
    "        makedir &= True\n",
    "    else:\n",
    "        makedir &= False\n",
    "\n",
    "if makedir:\n",
    "    print('-----------')\n",
    "    print('Creating new directory', model._name)\n",
    "    print('-----------')\n",
    "    os.mkdir(SAVE_PATH + model._name)\n",
    "    os.mkdir(SAVE_PATH + model._name + '/Metrics')\n",
    "    os.mkdir(SAVE_PATH + model._name + '/History')\n",
    "    os.mkdir(SAVE_PATH + model._name + '/H5')\n",
    "    \n",
    "\n",
    "SAVE_PATH = SAVE_PATH + model._name + '/'\n",
    "SAVE_PATH"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "background_save": true
    },
    "id": "iAODsiJWb8gK"
   },
   "outputs": [],
   "source": [
    "VALID_STEPS = len(valid_paths)//BATCH_SIZE\n",
    "print('-' * len(model._name))\n",
    "print(\"%s\" % model._name)\n",
    "print('-' * len(model._name))\n",
    " \n",
    "with strategy.scope():  \n",
    "    model.compile(\n",
    "        #optimizer = DemonAdam(T = STEPS_PER_EPOCH * EPOCHS),\n",
    "        optimizer = AdaBeliefOptimizer(0.0001, epsilon = 1e-7),\n",
    "        loss = bce_l1(),\n",
    "        metrics = [tf.keras.metrics.AUC(multi_label=True,name='auc')]\n",
    "    )\n",
    " \n",
    " \n",
    "    model_save = ModelCheckpoint('%sH5/%s.h5' % (SAVE_PATH, model._name), \n",
    "                                save_best_only = True, \n",
    "                                save_weights_only = True,\n",
    "                                monitor = 'val_loss', \n",
    "                                mode = 'min', verbose = 0)\n",
    " \n",
    "    early_stop = EarlyStopping(monitor = 'val_loss', min_delta = 0.0001, \n",
    "                                patience = 10, mode = 'min', verbose = 0,\n",
    "                                restore_best_weights = True)\n",
    "    \n",
    "    reduce_lr = ReduceLROnPlateau(monitor = 'val_loss', factor = 0.5, \n",
    "                                patience = 3, min_delta = 0.0001, \n",
    "                                mode = 'min', verbose = 0)\n",
    " \n",
    " \n",
    "    callbacks = [model_save, early_stop, reduce_lr]\n",
    " \n",
    " \n",
    "    history = model.fit(\n",
    "        train_df,\n",
    "        epochs=EPOCHS ,\n",
    "        batch_size=None,\n",
    "        validation_data=valid_df,\n",
    "        validation_steps=VALID_STEPS,\n",
    "        callbacks=callbacks,\n",
    "        shuffle=False,\n",
    "        verbose=1,\n",
    "        steps_per_epoch=STEPS_PER_EPOCH,\n",
    "        max_queue_size=BATCH_SIZE,\n",
    "        initial_epoch=0\n",
    "    )\n",
    " \n",
    "    hist_df = pd.DataFrame(history.history) \n",
    "    hist_df.to_csv(\"%sHistory/%s.csv\" % (SAVE_PATH, model._name))\n",
    " \n",
    " \n",
    "    metrics_train = compute_metrics(model, train_df, 'train', start_=0.5)\n",
    "    metrics_valid = compute_metrics(model, valid_df, 'valid', start_=0.5)\n",
    " \n",
    "    metrics = np.zeros((6,12), dtype=np.float32)\n",
    "    metrics[0] = metrics_train[0]\n",
    "    metrics[1] = metrics_valid[0]\n",
    "    metrics[2] = metrics_train[1]\n",
    "    metrics[3] = metrics_valid[1]\n",
    "    metrics[4] = metrics_train[2]\n",
    "    metrics[5] = metrics_valid[2]\n",
    "\n",
    "    cols = label_cols.to_list()\n",
    "    cols.append('Mean')\n",
    " \n",
    "    df = pd.DataFrame(metrics, columns=[cols], index=['AUC', 'AUC_valid', 'AMCC', 'AMCC_valid', 'pAMCC', 'pAMCC_valid'])\n",
    "    df.to_csv(\"%sMetrics/%s.csv\" % (SAVE_PATH, model._name), index = True)"
   ]
  }
 ],
 "metadata": {
  "accelerator": "TPU",
  "colab": {
   "collapsed_sections": [
    "FtzJCVBxGzIp",
    "c0GfBy6ZKq9V",
    "bOCwoQfIKtgu",
    "Otv5R5PfKu8H",
    "4I5fW2jQK0A0",
    "VfPiWPQbcQYA",
    "-5hHQ7oDKlRy",
    "nwtvzFukLFyS",
    "2u45U93hfLHi",
    "gH0k_CkLXfAb"
   ],
   "name": "Final.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
